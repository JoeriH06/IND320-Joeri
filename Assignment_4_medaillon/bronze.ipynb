{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd71161",
   "metadata": {},
   "source": [
    "# Medallion Pipeline – Bronze Layer (Elhub Energy Data)\n",
    "\n",
    "**Scope of this notebook (BRONZE only):**\n",
    "- Retrieve hourly **production** and **consumption** data from the Elhub Energy Data API\n",
    "  - Production: all price areas, years **2022–2024** (`PRODUCTION_PER_GROUP_MBA_HOUR`)\n",
    "  - Consumption: all price areas, years **2021–2024** (`CONSUMPTION_PER_GROUP_MBA_HOUR`)\n",
    "- Normalize the raw JSON into flat tabular form with minimal processing\n",
    "- Store raw-ish data to:\n",
    "  - Local bronze folder (CSV/Parquet)\n",
    "  - Cassandra (bronze tables)\n",
    "  - MongoDB (bronze collections)\n",
    "- No business rules, no joins, no aggregations → that’s for **silver** and **gold**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668b17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark unavailable: No module named 'pyspark.errors'\n",
      "Pandas version: 2.3.3\n",
      "Spark available: False\n"
     ]
    }
   ],
   "source": [
    "# Imports & Global Config, for the bronze level of this compulsory assignment\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spark and DB switches\n",
    "SPARK_AVAILABLE = False\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import (\n",
    "        StructType, StructField, StringType,\n",
    "        TimestampType, DoubleType, IntegerType\n",
    "    )\n",
    "    SPARK_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"Spark unavailable:\", e)\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Spark available:\", SPARK_AVAILABLE)\n",
    "\n",
    "# Elhub API basic config\n",
    "ELHUB_BASE = \"https://api.elhub.no/energy-data/v0\"\n",
    "ENTITY = \"price-areas\"\n",
    "\n",
    "DATASETS = {\n",
    "    \"production\":  \"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "    \"consumption\": \"CONSUMPTION_PER_GROUP_MBA_HOUR\",\n",
    "}\n",
    "\n",
    "ELHUB_AUTH_TOKEN = None\n",
    "\n",
    "# Time ranges (bronze)\n",
    "PROD_START = date(2022, 1, 1)\n",
    "PROD_END   = date(2024, 12, 31)\n",
    "\n",
    "CONS_START = date(2021, 1, 1)\n",
    "CONS_END   = date(2024, 12, 31)\n",
    "\n",
    "# Setting the local storage paths for the bronze level\n",
    "BRONZE_ROOT = \"../Data_Assignment_4/bronze\"\n",
    "BRONZE_PROD_DIR = os.path.join(BRONZE_ROOT, \"production\")\n",
    "BRONZE_CONS_DIR = os.path.join(BRONZE_ROOT, \"consumption\")\n",
    "\n",
    "os.makedirs(BRONZE_PROD_DIR, exist_ok=True)\n",
    "os.makedirs(BRONZE_CONS_DIR, exist_ok=True)\n",
    "\n",
    "# Cassandra & Mongo config (bronze)\n",
    "CASSANDRA_HOSTS = \"127.0.0.1\"\n",
    "CASSANDRA_KEYSPACE = \"energy\"\n",
    "CASSANDRA_TABLE_BRONZE_PROD = \"bronze_production_hourly\"\n",
    "CASSANDRA_TABLE_BRONZE_CONS = \"bronze_consumption_hourly\"\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "MONGO_DATABASE = \"ind320\"\n",
    "MONGO_COLL_BRONZE_PROD = \"bronze_production_hourly\"\n",
    "MONGO_COLL_BRONZE_CONS = \"bronze_consumption_hourly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the API headers here\n",
    "def elhub_headers():\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.api+json, application/json\",\n",
    "    }\n",
    "    if ELHUB_AUTH_TOKEN:\n",
    "        headers[\"Authorization\"] = ELHUB_AUTH_TOKEN\n",
    "    return headers\n",
    "\n",
    "# We return the ISO 8601 UTC format for datetime objects\n",
    "def iso_utc(d: datetime) -> str:\n",
    "    if d.tzinfo is None:\n",
    "        d = d.replace(tzinfo=timezone.utc)\n",
    "    else:\n",
    "        d = d.astimezone(timezone.utc)\n",
    "    return d.isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "# Then we ensure that the response is JSON\n",
    "def ensure_json(r: requests.Response):\n",
    "    ct = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    if \"json\" not in ct:\n",
    "        raise ValueError(f\"Non-JSON response (status {r.status_code}) from {r.url}\")\n",
    "    return r.json()\n",
    "\n",
    "# Here we fetch data from Elhub for a given dataset and time interval\n",
    "def fetch_interval_json(dataset_key: str, start_dt: datetime, end_dt: datetime):\n",
    "    dataset = DATASETS[dataset_key]\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"startTime\": iso_utc(start_dt),\n",
    "        \"endTime\": iso_utc(end_dt),\n",
    "        \"page[size]\": 10000,\n",
    "    }\n",
    "    url = f\"{ELHUB_BASE}/{ENTITY}\"\n",
    "    r = requests.get(url, headers=elhub_headers(), params=params, timeout=120)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(\n",
    "            f\"Elhub API error {r.status_code} for {dataset_key}: {r.text[:500]}\"\n",
    "        )\n",
    "    payload = ensure_json(r)\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33765a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we normalize the Elhub JSON payload into a pandas DataFrame\n",
    "def normalize_elhub_items(payload: dict, kind: str) -> pd.DataFrame:\n",
    "    data = payload.get(\"data\") or []\n",
    "    rows = []\n",
    "\n",
    "    for item in data:\n",
    "        attrs = (item or {}).get(\"attributes\") or {}\n",
    "        prod_series = attrs.get(\"productionPerGroupMbaHour\") or []\n",
    "        cons_series = attrs.get(\"consumptionPerGroupMbaHour\") or []\n",
    "\n",
    "        # Generic handler for series lists\n",
    "        def handle_series(series, is_production: bool):\n",
    "            for e in series:\n",
    "                row = {}\n",
    "                row[\"priceArea\"] = e.get(\"priceArea\") or attrs.get(\"name\")\n",
    "\n",
    "                if is_production:\n",
    "                    row[\"group\"] = e.get(\"productionGroup\")\n",
    "                    row[\"quantityKwh\"] = e.get(\"quantityKwh\")\n",
    "                    row[\"countMeteringPoints\"] = None\n",
    "                else:\n",
    "                    row[\"group\"] = e.get(\"consumptionGroup\")\n",
    "                    row[\"quantityKwh\"] = e.get(\"quantityKwh\")\n",
    "                    row[\"countMeteringPoints\"] = e.get(\"countMeteringPoints\")\n",
    "\n",
    "                row[\"startTime\"] = e.get(\"startTime\")\n",
    "                row[\"endTime\"] = e.get(\"endTime\")\n",
    "                rows.append(row)\n",
    "\n",
    "        if prod_series and kind == \"production\":\n",
    "            handle_series(prod_series, is_production=True)\n",
    "        if cons_series and kind == \"consumption\":\n",
    "            handle_series(cons_series, is_production=False)\n",
    "\n",
    "        # Fallback: sometimes a single timeseries dict instead of list\n",
    "        ts = attrs.get(\"timeseries\") or {}\n",
    "        if ts and not (prod_series or cons_series):\n",
    "            row = {}\n",
    "            row[\"priceArea\"] = ts.get(\"priceArea\") or attrs.get(\"name\")\n",
    "            if kind == \"production\":\n",
    "                row[\"group\"] = ts.get(\"productionGroup\")\n",
    "                row[\"quantityKwh\"] = ts.get(\"quantityKwh\")\n",
    "                row[\"countMeteringPoints\"] = None\n",
    "            else:\n",
    "                row[\"group\"] = ts.get(\"consumptionGroup\")\n",
    "                row[\"quantityKwh\"] = ts.get(\"quantityKwh\")\n",
    "                row[\"countMeteringPoints\"] = ts.get(\"countMeteringPoints\")\n",
    "\n",
    "            row[\"startTime\"] = ts.get(\"startTime\")\n",
    "            row[\"endTime\"] = ts.get(\"endTime\")\n",
    "            rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"priceArea\", \"group\", \"startTime\", \"endTime\",\n",
    "                     \"quantityKwh\", \"countMeteringPoints\"]\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Light typing (still bronze-friendly)\n",
    "    df[\"startTime\"] = pd.to_datetime(df[\"startTime\"], errors=\"coerce\", utc=True)\n",
    "    df[\"endTime\"] = pd.to_datetime(df[\"endTime\"], errors=\"coerce\", utc=True)\n",
    "    df[\"quantityKwh\"] = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    if \"countMeteringPoints\" in df.columns:\n",
    "        df[\"countMeteringPoints\"] = pd.to_numeric(df[\"countMeteringPoints\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d127115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to iterate months in a date range\n",
    "def iter_months(start_d: date, end_d: date):\n",
    "    cur = date(start_d.year, start_d.month, 1)\n",
    "    while cur <= end_d:\n",
    "        # next month\n",
    "        if cur.month == 12:\n",
    "            nxt = date(cur.year + 1, 1, 1)\n",
    "        else:\n",
    "            nxt = date(cur.year, cur.month + 1, 1)\n",
    "        yield cur, min(nxt - timedelta(days=1), end_d)\n",
    "        cur = nxt\n",
    "\n",
    "# We fetch & normalize data for given kind ('production' or 'consumption')\n",
    "def fetch_dataset_range(kind: str, start_d: date, end_d: date, pause: float = 1.0):\n",
    "    assert kind in (\"production\", \"consumption\")\n",
    "    frames = []\n",
    "    failures = []\n",
    "\n",
    "    for month_start, month_end in iter_months(start_d, end_d):\n",
    "        start_dt = datetime.combine(month_start, datetime.min.time()).replace(tzinfo=timezone.utc)\n",
    "        end_dt = datetime.combine(month_end + timedelta(days=1),\n",
    "                                  datetime.min.time()).replace(tzinfo=timezone.utc)\n",
    "\n",
    "        label = f\"{month_start:%Y-%m}\"\n",
    "        print(f\"Fetching {kind} for {label} ({start_dt} → {end_dt}) ...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            payload = fetch_interval_json(kind, start_dt, end_dt)\n",
    "            df_month = normalize_elhub_items(payload, kind=kind)\n",
    "            print(f\"{len(df_month):,} rows\")\n",
    "            if not df_month.empty:\n",
    "                frames.append(df_month)\n",
    "        except Exception as e:\n",
    "            print(\"FAILED:\", e)\n",
    "            failures.append((label, str(e)))\n",
    "\n",
    "        time.sleep(pause)\n",
    "\n",
    "    if frames:\n",
    "        df_all = pd.concat(frames, ignore_index=True)\n",
    "    else:\n",
    "        df_all = pd.DataFrame(\n",
    "            columns=[\"priceArea\", \"group\", \"startTime\", \"endTime\",\n",
    "                     \"quantityKwh\", \"countMeteringPoints\"]\n",
    "        )\n",
    "\n",
    "    return df_all, failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-related functions for the bronze level\n",
    "def build_spark(app_name=\"Elhub Bronze Ingestion\"):\n",
    "    if not SPARK_AVAILABLE:\n",
    "        raise RuntimeError(\"Spark is not available in this environment.\")\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .config(\"spark.cassandra.connection.host\", CASSANDRA_HOSTS)\n",
    "        .config(\"spark.mongodb.write.connection.uri\", MONGO_URI)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "\n",
    "def to_spark(df: pd.DataFrame):\n",
    "    if not SPARK_AVAILABLE:\n",
    "        raise RuntimeError(\"Spark is not available.\")\n",
    "    schema = StructType([\n",
    "        StructField(\"priceArea\", StringType(), True),\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"startTime\", TimestampType(), True),\n",
    "        StructField(\"endTime\", TimestampType(), True),\n",
    "        StructField(\"quantityKwh\", DoubleType(), True),\n",
    "        StructField(\"countMeteringPoints\", IntegerType(), True),\n",
    "    ])\n",
    "    spark = build_spark()\n",
    "    return spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "\n",
    "def write_bronze_to_cassandra(sdf, table_name: str):\n",
    "    (\n",
    "        sdf.write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(keyspace=CASSANDRA_KEYSPACE, table=table_name)\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "\n",
    "def write_bronze_to_mongo(sdf, collection_name: str):\n",
    "    (\n",
    "        sdf.write\n",
    "        .format(\"mongodb\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"database\", MONGO_DATABASE)\n",
    "        .option(\"collection\", collection_name)\n",
    "        .save()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching production for 2022-01 (2022-01-01 00:00:00+00:00 → 2022-02-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-02 (2022-02-01 00:00:00+00:00 → 2022-03-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-03 (2022-03-01 00:00:00+00:00 → 2022-04-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-04 (2022-04-01 00:00:00+00:00 → 2022-05-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-05 (2022-05-01 00:00:00+00:00 → 2022-06-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-06 (2022-06-01 00:00:00+00:00 → 2022-07-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-07 (2022-07-01 00:00:00+00:00 → 2022-08-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-08 (2022-08-01 00:00:00+00:00 → 2022-09-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-09 (2022-09-01 00:00:00+00:00 → 2022-10-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-10 (2022-10-01 00:00:00+00:00 → 2022-11-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-11 (2022-11-01 00:00:00+00:00 → 2022-12-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2022-12 (2022-12-01 00:00:00+00:00 → 2023-01-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-01 (2023-01-01 00:00:00+00:00 → 2023-02-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-02 (2023-02-01 00:00:00+00:00 → 2023-03-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-03 (2023-03-01 00:00:00+00:00 → 2023-04-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-04 (2023-04-01 00:00:00+00:00 → 2023-05-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-05 (2023-05-01 00:00:00+00:00 → 2023-06-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-06 (2023-06-01 00:00:00+00:00 → 2023-07-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-07 (2023-07-01 00:00:00+00:00 → 2023-08-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-08 (2023-08-01 00:00:00+00:00 → 2023-09-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-09 (2023-09-01 00:00:00+00:00 → 2023-10-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-10 (2023-10-01 00:00:00+00:00 → 2023-11-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-11 (2023-11-01 00:00:00+00:00 → 2023-12-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2023-12 (2023-12-01 00:00:00+00:00 → 2024-01-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-01 (2024-01-01 00:00:00+00:00 → 2024-02-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-02 (2024-02-01 00:00:00+00:00 → 2024-03-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-03 (2024-03-01 00:00:00+00:00 → 2024-04-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-04 (2024-04-01 00:00:00+00:00 → 2024-05-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-05 (2024-05-01 00:00:00+00:00 → 2024-06-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-06 (2024-06-01 00:00:00+00:00 → 2024-07-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-07 (2024-07-01 00:00:00+00:00 → 2024-08-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-08 (2024-08-01 00:00:00+00:00 → 2024-09-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-09 (2024-09-01 00:00:00+00:00 → 2024-10-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-10 (2024-10-01 00:00:00+00:00 → 2024-11-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-11 (2024-11-01 00:00:00+00:00 → 2024-12-01 00:00:00+00:00) ... 18,851 rows\n",
      "Fetching production for 2024-12 (2024-12-01 00:00:00+00:00 → 2025-01-01 00:00:00+00:00) ... 18,851 rows\n",
      "\n",
      "Production fetch done.\n",
      "Rows: 678636\n",
      "Failures: []\n",
      "Years present in production: [np.int32(2025)]\n",
      "Saved 678,636 rows to ../Data_Assignment_4/bronze\\production\\production_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Now we perform the bronze ingestion: PRODUCTION 2022–2024\n",
    "prod_df, prod_fail = fetch_dataset_range(\"production\", PROD_START, PROD_END)\n",
    "\n",
    "print(\"\\nProduction fetch done.\")\n",
    "print(\"Rows:\", len(prod_df))\n",
    "print(\"Failures:\", prod_fail[:5])\n",
    "\n",
    "# And we save the production data locally, partitioned by year\n",
    "if not prod_df.empty:\n",
    "    prod_df = prod_df.copy()\n",
    "    prod_df[\"year\"] = prod_df[\"startTime\"].dt.year\n",
    "    print(\"Years present in production:\", sorted(prod_df[\"year\"].dropna().unique()))\n",
    "\n",
    "    for year, df_y in prod_df.groupby(\"year\"):\n",
    "        out_path = os.path.join(BRONZE_PROD_DIR, f\"production_{int(year)}.csv\")\n",
    "        df_y.drop(columns=[\"year\"]).to_csv(out_path, index=False)\n",
    "        print(f\"Saved {len(df_y):,} rows to {out_path}\")\n",
    "else:\n",
    "    print(\"⚠ No production rows fetched – nothing saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdc1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod_df dtypes:\n",
      " priceArea                           object\n",
      "group                               object\n",
      "quantityKwh                        float64\n",
      "countMeteringPoints                float64\n",
      "startTime              datetime64[ns, UTC]\n",
      "endTime                datetime64[ns, UTC]\n",
      "year                                 int32\n",
      "dtype: object\n",
      "\n",
      "startTime sample:\n",
      "0   2025-10-22 19:00:00+00:00\n",
      "1   2025-10-22 20:00:00+00:00\n",
      "2   2025-10-22 21:00:00+00:00\n",
      "3   2025-10-22 22:00:00+00:00\n",
      "4   2025-10-22 23:00:00+00:00\n",
      "Name: startTime, dtype: datetime64[ns, UTC]\n",
      "\n",
      "Unique years in startTime:\n",
      "[np.int32(2025)]\n"
     ]
    }
   ],
   "source": [
    "# We do some quick checks on the production DataFrame\n",
    "print(\"prod_df dtypes:\\n\", prod_df.dtypes)\n",
    "print(\"\\nstartTime sample:\")\n",
    "print(prod_df[\"startTime\"].head())\n",
    "\n",
    "if not prod_df.empty: \n",
    "    print(\"\\nUnique years in startTime:\")\n",
    "    print(sorted(prod_df[\"startTime\"].dt.year.dropna().unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dce112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching consumption for 2021-01 (2021-01-01 00:00:00+00:00 → 2021-02-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-02 (2021-02-01 00:00:00+00:00 → 2021-03-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-03 (2021-03-01 00:00:00+00:00 → 2021-04-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-04 (2021-04-01 00:00:00+00:00 → 2021-05-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-05 (2021-05-01 00:00:00+00:00 → 2021-06-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-06 (2021-06-01 00:00:00+00:00 → 2021-07-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-07 (2021-07-01 00:00:00+00:00 → 2021-08-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-08 (2021-08-01 00:00:00+00:00 → 2021-09-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-09 (2021-09-01 00:00:00+00:00 → 2021-10-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-10 (2021-10-01 00:00:00+00:00 → 2021-11-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-11 (2021-11-01 00:00:00+00:00 → 2021-12-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2021-12 (2021-12-01 00:00:00+00:00 → 2022-01-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-01 (2022-01-01 00:00:00+00:00 → 2022-02-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-02 (2022-02-01 00:00:00+00:00 → 2022-03-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-03 (2022-03-01 00:00:00+00:00 → 2022-04-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-04 (2022-04-01 00:00:00+00:00 → 2022-05-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-05 (2022-05-01 00:00:00+00:00 → 2022-06-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-06 (2022-06-01 00:00:00+00:00 → 2022-07-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-07 (2022-07-01 00:00:00+00:00 → 2022-08-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-08 (2022-08-01 00:00:00+00:00 → 2022-09-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-09 (2022-09-01 00:00:00+00:00 → 2022-10-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-10 (2022-10-01 00:00:00+00:00 → 2022-11-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-11 (2022-11-01 00:00:00+00:00 → 2022-12-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2022-12 (2022-12-01 00:00:00+00:00 → 2023-01-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-01 (2023-01-01 00:00:00+00:00 → 2023-02-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-02 (2023-02-01 00:00:00+00:00 → 2023-03-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-03 (2023-03-01 00:00:00+00:00 → 2023-04-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-04 (2023-04-01 00:00:00+00:00 → 2023-05-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-05 (2023-05-01 00:00:00+00:00 → 2023-06-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-06 (2023-06-01 00:00:00+00:00 → 2023-07-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-07 (2023-07-01 00:00:00+00:00 → 2023-08-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-08 (2023-08-01 00:00:00+00:00 → 2023-09-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-09 (2023-09-01 00:00:00+00:00 → 2023-10-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-10 (2023-10-01 00:00:00+00:00 → 2023-11-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-11 (2023-11-01 00:00:00+00:00 → 2023-12-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2023-12 (2023-12-01 00:00:00+00:00 → 2024-01-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-01 (2024-01-01 00:00:00+00:00 → 2024-02-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-02 (2024-02-01 00:00:00+00:00 → 2024-03-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-03 (2024-03-01 00:00:00+00:00 → 2024-04-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-04 (2024-04-01 00:00:00+00:00 → 2024-05-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-05 (2024-05-01 00:00:00+00:00 → 2024-06-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-06 (2024-06-01 00:00:00+00:00 → 2024-07-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-07 (2024-07-01 00:00:00+00:00 → 2024-08-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-08 (2024-08-01 00:00:00+00:00 → 2024-09-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-09 (2024-09-01 00:00:00+00:00 → 2024-10-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-10 (2024-10-01 00:00:00+00:00 → 2024-11-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-11 (2024-11-01 00:00:00+00:00 → 2024-12-01 00:00:00+00:00) ... 18,100 rows\n",
      "Fetching consumption for 2024-12 (2024-12-01 00:00:00+00:00 → 2025-01-01 00:00:00+00:00) ... 18,100 rows\n",
      "\n",
      "Consumption fetch done.\n",
      "Rows: 868800\n",
      "Failures: []\n",
      "Years present in consumption: [np.int32(2025)]\n",
      "Saved 868,800 rows to ../Data_Assignment_4/bronze\\consumption\\consumption_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# and we do the same for the CONSUMPTION data: 2021–2024\n",
    "cons_df, cons_fail = fetch_dataset_range(\"consumption\", CONS_START, CONS_END)\n",
    "\n",
    "print(\"\\nConsumption fetch done.\")\n",
    "print(\"Rows:\", len(cons_df))\n",
    "print(\"Failures:\", cons_fail[:5])\n",
    "\n",
    "# And we save the consumption data locally, partitioned by year\n",
    "if not cons_df.empty:\n",
    "    cons_df = cons_df.copy()\n",
    "    cons_df[\"year\"] = cons_df[\"startTime\"].dt.year\n",
    "    print(\"Years present in consumption:\", sorted(cons_df[\"year\"].dropna().unique()))\n",
    "\n",
    "    for year, df_y in cons_df.groupby(\"year\"):\n",
    "        out_path = os.path.join(BRONZE_CONS_DIR, f\"consumption_{int(year)}.csv\")\n",
    "        df_y.drop(columns=[\"year\"]).to_csv(out_path, index=False)\n",
    "        print(f\"Saved {len(df_y):,} rows to {out_path}\")\n",
    "else:\n",
    "    print(\"⚠ No consumption rows fetched – nothing saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f54906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production time span: 2025-10-22 19:00:00+00:00 → 2025-11-21 22:00:00+00:00\n",
      "  priceArea  group  quantityKwh  countMeteringPoints  \\\n",
      "0       NO1  hydro    2332513.0                  NaN   \n",
      "1       NO1  hydro    2281520.8                  NaN   \n",
      "2       NO1  hydro    2308167.5                  NaN   \n",
      "3       NO1  hydro    2253987.5                  NaN   \n",
      "4       NO1  hydro    2241552.0                  NaN   \n",
      "\n",
      "                  startTime                   endTime  year  \n",
      "0 2025-10-22 19:00:00+00:00 2025-10-22 20:00:00+00:00  2025  \n",
      "1 2025-10-22 20:00:00+00:00 2025-10-22 21:00:00+00:00  2025  \n",
      "2 2025-10-22 21:00:00+00:00 2025-10-22 22:00:00+00:00  2025  \n",
      "3 2025-10-22 22:00:00+00:00 2025-10-22 23:00:00+00:00  2025  \n",
      "4 2025-10-22 23:00:00+00:00 2025-10-23 00:00:00+00:00  2025  \n",
      "Consumption time span: 2025-10-22 19:00:00+00:00 → 2025-11-21 22:00:00+00:00\n",
      "  priceArea  group  quantityKwh  countMeteringPoints  \\\n",
      "0       NO1  cabin    67069.060                  NaN   \n",
      "1       NO1  cabin    66767.110                  NaN   \n",
      "2       NO1  cabin    65964.305                  NaN   \n",
      "3       NO1  cabin    65159.824                  NaN   \n",
      "4       NO1  cabin    64141.660                  NaN   \n",
      "\n",
      "                  startTime                   endTime  year  \n",
      "0 2025-10-22 19:00:00+00:00 2025-10-22 20:00:00+00:00  2025  \n",
      "1 2025-10-22 20:00:00+00:00 2025-10-22 21:00:00+00:00  2025  \n",
      "2 2025-10-22 21:00:00+00:00 2025-10-22 22:00:00+00:00  2025  \n",
      "3 2025-10-22 22:00:00+00:00 2025-10-22 23:00:00+00:00  2025  \n",
      "4 2025-10-22 23:00:00+00:00 2025-10-23 00:00:00+00:00  2025  \n"
     ]
    }
   ],
   "source": [
    "# We do some quick checks on the consumption DataFrame\n",
    "# Checking for consumption DataFrame\n",
    "if not prod_df.empty:\n",
    "    print(\"Production time span:\",\n",
    "          prod_df[\"startTime\"].min(), \"→\", prod_df[\"startTime\"].max())\n",
    "    print(prod_df.head())\n",
    "\n",
    "if not cons_df.empty:\n",
    "    print(\"Consumption time span:\",\n",
    "          cons_df[\"startTime\"].min(), \"→\", cons_df[\"startTime\"].max())\n",
    "    print(cons_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
